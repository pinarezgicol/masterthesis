{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AdCppuKpdq1x",
    "outputId": "1277a5f7-38e9-43c6-cd9b-2ac12780f7bc"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Input, Lambda, Layer\n",
    "from keras.regularizers import l2\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import model_from_yaml\n",
    "from modAL.models import ActiveLearner\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.backend import eager_learning_phase_scope\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# data load\n",
    "(x_tr, y_tr), (x_te, y_te) = mnist.load_data()\n",
    "x_tr, x_te = x_tr.astype('float32')/255., x_te.astype('float32')/255.\n",
    "x_tr, x_te = x_tr.reshape(x_tr.shape[0], -1), x_te.reshape(x_te.shape[0], -1)\n",
    "print(x_tr.shape, x_te.shape)\n",
    "\n",
    "# network parameters\n",
    "batch_size, n_epoch = 64, 30\n",
    "n_hidden, z_dim = 512, 32\n",
    "epsilon_std = 1.0\n",
    "epochs = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output custom_variational_layer missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to custom_variational_layer.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 784)]        0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          401920      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 20)           10260       ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 20)           10260       ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 20)           0           ['dense_1[0][0]',                \n",
      "                                                                  'dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 512)          10752       ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 784)          402192      ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " custom_variational_layer (Cust  (None, 784)         0           ['input_1[0][0]',                \n",
      " omVariationalLayer)                                              'dense_4[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 835,384\n",
      "Trainable params: 835,384\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "original_dim = 784\n",
    "latent_dim = 20\n",
    "intermediate_dim = 512\n",
    "epochs = 60\n",
    "epsilon_std = 1.0\n",
    "\n",
    "# encoder architecture\n",
    "x = Input(shape=(original_dim,))\n",
    "encoder_h = Dense(intermediate_dim, activation='relu')(x)\n",
    "z_mean = Dense(latent_dim)(encoder_h)\n",
    "z_log_var = Dense(latent_dim)(encoder_h)\n",
    "\n",
    "# sampling layer from latent distribution\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "#z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "z = Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "# decoder / generator architecture\n",
    "decoder_h = Dense(intermediate_dim, activation='relu')\n",
    "decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "# Custom loss layer\n",
    "class CustomVariationalLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean):\n",
    "        xent_loss = original_dim * keras.metrics.binary_crossentropy(x, x_decoded_mean)\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return K.mean(xent_loss + kl_loss)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded_mean = inputs[1]\n",
    "        loss = self.vae_loss(x, x_decoded_mean)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # We won't actually use the output.\n",
    "        return x\n",
    "    \n",
    "y = CustomVariationalLayer()([x, x_decoded_mean])\n",
    "\n",
    "# entire model\n",
    "vae = Model(x, y)\n",
    "vae.compile(optimizer='adam', loss=None)\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/60\n",
      "59392/60000 [============================>.] - ETA: 0s - loss: 184.6309"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\pinar\\onedrive\\masaüstü\\masterthesis\\msthesisenv\\lib\\site-packages\\keras\\engine\\training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 69us/sample - loss: 184.1973 - val_loss: 138.4864\n",
      "Epoch 2/60\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 129.8368 - val_loss: 122.2692\n",
      "Epoch 3/60\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 120.0936 - val_loss: 116.5941\n",
      "Epoch 4/60\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 115.6476 - val_loss: 113.2830\n",
      "Epoch 5/60\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 113.0324 - val_loss: 111.1630\n",
      "Epoch 6/60\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 111.2060 - val_loss: 109.6368\n",
      "Epoch 7/60\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 109.8988 - val_loss: 108.8003\n",
      "Epoch 8/60\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 108.9421 - val_loss: 107.9411\n",
      "Epoch 9/60\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 108.1346 - val_loss: 107.1655\n",
      "Epoch 10/60\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 107.5405 - val_loss: 106.6536\n",
      "Epoch 11/60\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 106.9895 - val_loss: 106.1510\n",
      "Epoch 12/60\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 106.5190 - val_loss: 105.9290\n",
      "Epoch 13/60\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 106.1713 - val_loss: 105.4699\n",
      "Epoch 14/60\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 105.7894 - val_loss: 105.4103\n",
      "Epoch 15/60\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 105.5134 - val_loss: 105.0607\n",
      "Epoch 16/60\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 105.2320 - val_loss: 104.7735\n",
      "Epoch 17/60\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 104.9765 - val_loss: 104.6745\n",
      "Epoch 18/60\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 104.7945 - val_loss: 104.4988\n",
      "Epoch 19/60\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 104.5850 - val_loss: 104.3560\n",
      "Epoch 20/60\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 104.3995 - val_loss: 104.2647\n",
      "Epoch 21/60\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 104.2331 - val_loss: 103.9571\n",
      "Epoch 22/60\n",
      "27648/60000 [============>.................] - ETA: 1s - loss: 104.3450"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19280/2138673230.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m history = vae.fit(x_tr,\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pinar\\onedrive\\masaüstü\\masterthesis\\msthesisenv\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_select_training_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m     return func.fit(\n\u001b[0m\u001b[0;32m    778\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pinar\\onedrive\\masaüstü\\masterthesis\\msthesisenv\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    638\u001b[0m       \u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_sample_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 640\u001b[1;33m     return fit_loop(\n\u001b[0m\u001b[0;32m    641\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pinar\\onedrive\\masaüstü\\masterthesis\\msthesisenv\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pinar\\onedrive\\masaüstü\\masterthesis\\msthesisenv\\lib\\site-packages\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   4184\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4186\u001b[1;33m     fetched = self._callable_fn(*array_vals,\n\u001b[0m\u001b[0;32m   4187\u001b[0m                                 run_metadata=self.run_metadata)\n\u001b[0;32m   4188\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pinar\\onedrive\\masaüstü\\masterthesis\\msthesisenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1481\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1482\u001b[0m         \u001b[0mrun_metadata_ptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_NewBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1483\u001b[1;33m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0m\u001b[0;32m   1484\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1485\u001b[0m                                                run_metadata_ptr)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "history = vae.fit(x_tr,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_te, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build encoder\n",
    "encoder = Model(x, z_mean)\n",
    "encoder.summary()\n",
    "\n",
    "# Plot of the digit classes in the latent space\n",
    "x_te_latent = encoder.predict(x_te, batch_size=batch_size)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_te_latent[:, 0], x_te_latent[:, 1], c=y_te)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_final = TSNE(perplexity=30).fit_transform(x_te_latent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "import matplotlib.patheffects as pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With the above line, our job is done. But why did we even reduce the dimensions in the first place?\n",
    "#To visualise it on a graph.\n",
    "\n",
    "#So, here is a utility function that helps to do a scatter plot of thee transformed data \n",
    "\n",
    "def plot(x, colors):\n",
    "  \n",
    "    palette = np.array(sb.color_palette(\"hls\", 10))  #Choosing color palette \n",
    "\n",
    "    # Create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40, c=palette[colors.astype(np.int)])\n",
    "    # Add the labels for each digit.\n",
    "    txts = []\n",
    "    for i in range(10):\n",
    "        # Position of each label.\n",
    "        xtext, ytext = np.median(x[colors == i, :], axis=0)\n",
    "        txt = ax.text(xtext, ytext, str(i), fontsize=24)\n",
    "        txt.set_path_effects([pe.Stroke(linewidth=5, foreground=\"w\"), pe.Normal()])\n",
    "        txts.append(txt)\n",
    "    return f, ax, txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(digits_final,y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save(\"mnist_vae_encoder_32.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dbal_keras_different_latent_sizes.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "msthesisenv",
   "language": "python",
   "name": "msthesisenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
