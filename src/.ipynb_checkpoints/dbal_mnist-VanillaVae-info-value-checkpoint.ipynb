{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import MNIST\n",
    "from skorch import NeuralNetClassifier\n",
    "from modAL.models import ActiveLearner\n",
    "from batchbald_redux import batchbald\n",
    "from acquisition_functions import *\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from keras.layers import Input, Dense, Lambda, Layer\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIMS = [64]\n",
    "MAX_EPOCHS = 200\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "EXPERIMENT_COUNT = 1\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "HIDDEN_DIM = 512\n",
    "ORIGINAL_DIM = 784\n",
    "results_path = 'results\\\\mnist_vanillavae_dbal'\n",
    "epsilon_std = 1.0\n",
    "\n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path)\n",
    "\n",
    "ACQ_FUNCS = {\n",
    "    \"bald\": bald_info_value_vanillavae\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_REG(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(MLP_REG, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(128, 10),)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "\n",
    "X_test = (X_test.astype(np.float32) - 127.5) / 127.5\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_learning_procedure(query_strategy,\n",
    "                              X_test,\n",
    "                              y_test,\n",
    "                              X_pool,\n",
    "                              y_pool,\n",
    "                              X_initial,\n",
    "                              y_initial,\n",
    "                              estimator,\n",
    "                              n_queries=98,\n",
    "                              n_instances=10):\n",
    "    learner = ActiveLearner(estimator=estimator,\n",
    "                            X_training=X_initial,\n",
    "                            y_training=y_initial,\n",
    "                            query_strategy=query_strategy,\n",
    "                           )\n",
    "    perf_hist = [learner.score(X_test, y_test)]\n",
    "    active_pool_size = [len(X_initial)]\n",
    "    pool_size = len(X_initial)\n",
    "    for index in range(n_queries):\n",
    "        query_idx, query_instance = learner.query(X_pool, n_instances)\n",
    "        learner.teach(X_pool[query_idx], y_pool[query_idx])\n",
    "        X_pool = np.delete(X_pool, query_idx, axis=0)\n",
    "        y_pool = np.delete(y_pool, query_idx, axis=0)\n",
    "        model_accuracy = learner.score(X_test, y_test)\n",
    "        pool_size = pool_size + n_instances\n",
    "        print('Accuracy after query {n}: {acc:0.4f}'.format(n=index + 1, acc=model_accuracy))\n",
    "        perf_hist.append(model_accuracy)\n",
    "        active_pool_size.append(pool_size)\n",
    "    return perf_hist, active_pool_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after query 1: 0.5647\n",
      "Accuracy after query 2: 0.6424\n",
      "Accuracy after query 3: 0.6414\n",
      "Accuracy after query 4: 0.6902\n",
      "Accuracy after query 5: 0.6921\n",
      "Accuracy after query 6: 0.7205\n",
      "Accuracy after query 7: 0.7288\n",
      "Accuracy after query 8: 0.7273\n",
      "Accuracy after query 9: 0.7448\n",
      "Accuracy after query 10: 0.7484\n",
      "Accuracy after query 11: 0.7800\n",
      "Accuracy after query 12: 0.7785\n",
      "Accuracy after query 13: 0.7479\n",
      "Accuracy after query 14: 0.7660\n",
      "Accuracy after query 15: 0.7916\n",
      "Accuracy after query 16: 0.8043\n",
      "Accuracy after query 17: 0.7885\n",
      "Accuracy after query 18: 0.8193\n",
      "Accuracy after query 19: 0.8098\n",
      "Accuracy after query 20: 0.8083\n",
      "Accuracy after query 21: 0.8202\n",
      "Accuracy after query 22: 0.8206\n",
      "Accuracy after query 23: 0.8334\n",
      "Accuracy after query 24: 0.8451\n",
      "Accuracy after query 25: 0.8302\n",
      "Accuracy after query 26: 0.8474\n",
      "Accuracy after query 27: 0.8240\n",
      "Accuracy after query 28: 0.8337\n",
      "Accuracy after query 29: 0.8438\n",
      "Accuracy after query 30: 0.8398\n",
      "Accuracy after query 31: 0.8352\n",
      "Accuracy after query 32: 0.8324\n",
      "Accuracy after query 33: 0.8502\n",
      "Accuracy after query 34: 0.8546\n",
      "Accuracy after query 35: 0.8639\n",
      "Accuracy after query 36: 0.8628\n",
      "Accuracy after query 37: 0.8657\n",
      "Accuracy after query 38: 0.8724\n",
      "Accuracy after query 39: 0.8803\n",
      "Accuracy after query 40: 0.8720\n",
      "Accuracy after query 41: 0.8592\n",
      "Accuracy after query 42: 0.8681\n",
      "Accuracy after query 43: 0.8712\n",
      "Accuracy after query 44: 0.8850\n",
      "Accuracy after query 45: 0.8847\n",
      "Accuracy after query 46: 0.8857\n",
      "Accuracy after query 47: 0.8774\n",
      "Accuracy after query 48: 0.8897\n",
      "Accuracy after query 49: 0.8824\n",
      "Accuracy after query 50: 0.8834\n",
      "Accuracy after query 51: 0.8843\n",
      "Accuracy after query 52: 0.8988\n",
      "Accuracy after query 53: 0.8956\n",
      "Accuracy after query 54: 0.8849\n",
      "Accuracy after query 55: 0.8933\n",
      "Accuracy after query 56: 0.8885\n",
      "Accuracy after query 57: 0.8857\n",
      "Accuracy after query 58: 0.8920\n",
      "Accuracy after query 59: 0.8950\n",
      "Accuracy after query 60: 0.9098\n",
      "Accuracy after query 61: 0.9075\n",
      "Accuracy after query 62: 0.8963\n",
      "Accuracy after query 63: 0.9005\n",
      "Accuracy after query 64: 0.9116\n",
      "Accuracy after query 65: 0.9086\n",
      "Accuracy after query 66: 0.9053\n",
      "Accuracy after query 67: 0.8932\n",
      "Accuracy after query 68: 0.9090\n",
      "Accuracy after query 69: 0.9075\n",
      "Accuracy after query 70: 0.9079\n",
      "Accuracy after query 71: 0.9169\n",
      "Accuracy after query 72: 0.9159\n",
      "Accuracy after query 73: 0.9103\n",
      "Accuracy after query 74: 0.9162\n",
      "Accuracy after query 75: 0.9228\n",
      "Accuracy after query 76: 0.9145\n",
      "Accuracy after query 77: 0.9157\n",
      "Accuracy after query 78: 0.9289\n",
      "Accuracy after query 79: 0.9227\n",
      "Accuracy after query 80: 0.9215\n",
      "Accuracy after query 81: 0.9260\n",
      "Accuracy after query 82: 0.9260\n",
      "Accuracy after query 83: 0.9215\n",
      "Accuracy after query 84: 0.9272\n",
      "Accuracy after query 85: 0.9228\n",
      "Accuracy after query 86: 0.9272\n",
      "Accuracy after query 87: 0.9303\n",
      "Accuracy after query 88: 0.9273\n",
      "Accuracy after query 89: 0.9358\n",
      "Accuracy after query 90: 0.9267\n",
      "Accuracy after query 91: 0.9205\n",
      "Accuracy after query 92: 0.9276\n",
      "Accuracy after query 93: 0.9250\n",
      "Accuracy after query 94: 0.9247\n",
      "Accuracy after query 95: 0.9278\n",
      "Accuracy after query 96: 0.9324\n",
      "Accuracy after query 97: 0.9291\n",
      "Accuracy after query 98: 0.9330\n"
     ]
    }
   ],
   "source": [
    "for latent_dim in LATENT_DIMS:\n",
    "\n",
    "    # encoder architecture\n",
    "    x = Input(shape=(ORIGINAL_DIM,))\n",
    "    encoder_h = Dense(HIDDEN_DIM, activation='relu')(x)\n",
    "    z_mean = Dense(latent_dim)(encoder_h)\n",
    "\n",
    "    # encoder to generate latent variables from input\n",
    "    encoder = Model(x, z_mean)\n",
    "\n",
    "    encoder.load_weights(r\"C:\\Users\\pinar\\OneDrive\\Masaüstü\\masterthesis\\src\\Generative Models\\mnist\\mnist_vanillavae\\mnist_vanillavae_z_mean_{latent_dim}.h5\".format(latent_dim=latent_dim))\n",
    "    \n",
    "    X_train_enhanced = encoder.predict(X_train, batch_size=32)\n",
    "    X_test_enhanced = encoder.predict(X_test, batch_size=32)\n",
    "    \n",
    "    for exp_iter in range(EXPERIMENT_COUNT):\n",
    "        np.random.seed(exp_iter)\n",
    "        initial_idx = np.array([],dtype=int)\n",
    "        for i in range(10):\n",
    "            idx = np.random.choice(np.where(y_train==i)[0], size=2, replace=False)\n",
    "            initial_idx = np.concatenate((initial_idx, idx))\n",
    "\n",
    "        for func_name, acquisition_func in ACQ_FUNCS.items():  \n",
    "\n",
    "            X_initial = X_train_enhanced[initial_idx]\n",
    "            y_initial = y_train[initial_idx]\n",
    "\n",
    "            X_pool = np.delete(X_train_enhanced, initial_idx, axis=0)\n",
    "            y_pool = np.delete(y_train, initial_idx, axis=0)\n",
    "\n",
    "            model = MLP_REG(latent_dim).to(DEVICE)\n",
    "\n",
    "            estimator = NeuralNetClassifier(model,\n",
    "                                          max_epochs=MAX_EPOCHS,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          lr=LEARNING_RATE,\n",
    "                                          optimizer=torch.optim.Adam,\n",
    "                                          criterion=torch.nn.CrossEntropyLoss,\n",
    "                                          train_split=None,\n",
    "                                          verbose=0,\n",
    "                                          device=DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "            acc_arr, dataset_size_arr = active_learning_procedure(acquisition_func,\n",
    "                                                              X_test_enhanced,\n",
    "                                                              y_test,\n",
    "                                                              X_pool,\n",
    "                                                              y_pool,\n",
    "                                                              X_initial,\n",
    "                                                              y_initial,\n",
    "                                                              estimator,)\n",
    "            file_name = os.path.join(results_path, \"{func_name}_latent_dim_{latent_dim}_exp_{exp_iter}.npy\".format(func_name=func_name, exp_iter=exp_iter, latent_dim=latent_dim))\n",
    "            np.save(file_name, (acc_arr, dataset_size_arr))\n",
    "        '''\n",
    "        for func_name, acquisition_func in ACQ_FUNCS.items():  \n",
    "            X_initial = X_train[initial_idx]\n",
    "            y_initial = y_train[initial_idx]\n",
    "\n",
    "            X_pool = np.delete(X_train, initial_idx, axis=0)\n",
    "            y_pool = np.delete(y_train, initial_idx, axis=0)\n",
    "\n",
    "            model = MLP_REG(ORIGINAL_DIM).to(DEVICE)\n",
    "\n",
    "            estimator = NeuralNetClassifier(model,\n",
    "                                          max_epochs=MAX_EPOCHS,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          lr=LEARNING_RATE,\n",
    "                                          optimizer=torch.optim.Adam,\n",
    "                                          criterion=torch.nn.CrossEntropyLoss,\n",
    "                                          train_split=None,\n",
    "                                          verbose=0,\n",
    "                                          device=DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "            acc_arr, dataset_size_arr = active_learning_procedure(acquisition_func,\n",
    "                                                              X_test,\n",
    "                                                              y_test,\n",
    "                                                              X_pool,\n",
    "                                                              y_pool,\n",
    "                                                              X_initial,\n",
    "                                                              y_initial,\n",
    "                                                              estimator,)\n",
    "            file_name = os.path.join(\"results\\\\mnist_dbal\", \"{func_name}_exp_{exp_iter}.npy\".format(func_name=func_name, exp_iter=exp_iter))\n",
    "            np.save(file_name, (acc_arr, dataset_size_arr))\n",
    "            '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msthesisenv",
   "language": "python",
   "name": "msthesisenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
