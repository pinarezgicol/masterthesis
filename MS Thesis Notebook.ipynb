{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from pprint import pprint\n",
    "\n",
    "import torch.cuda\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "\n",
    "from baal import ActiveLearningDataset, ModelWrapper\n",
    "from baal.active import ActiveLearningLoop\n",
    "from baal.active.heuristics import BALD\n",
    "from baal.bayesian.dropout import patch_module\n",
    "from baal.utils.metrics import Accuracy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "NO_OF_INITIAL_LABELLED = 20\n",
    "DROPOUT_RATE = 0.4\n",
    "EPOCH = 50\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.RandomRotation(30), transforms.ToTensor()])\n",
    "test_transform = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_ds = MNIST(\"/dataset_mnist\", train=True, transform=train_transform, download=True)\n",
    "test_ds = MNIST(\"/dataset_mnist\", train=False, transform=test_transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "al_dataset = ActiveLearningDataset(train_ds, pool_specifics={\"transform\": test_transform})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_uniformly(al_dataset, no_per_class=2):\n",
    "    initial_dataset_size = al_dataset.labelled.shape[0]\n",
    "    indices_to_be_labelled = set()\n",
    "    \n",
    "    for class_label in range(10):\n",
    "        n_iter = 0\n",
    "        while(n_iter < no_per_class):\n",
    "            idx = np.random.choice(initial_dataset_size, 1)[0]\n",
    "            selected_label = al_dataset.get_raw(idx)[1]\n",
    "            if idx not in indices_to_be_labelled and selected_label == class_label:\n",
    "                indices_to_be_labelled.add(idx)\n",
    "                n_iter = n_iter + 1\n",
    "    #print(indices_to_be_labelled)\n",
    "    for elt in indices_to_be_labelled:\n",
    "        #print(elt.item())\n",
    "        al_dataset.label(elt.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "al_dataset = ActiveLearningDataset(train_ds, pool_specifics={\"transform\": test_transform})\n",
    "label_uniformly(al_dataset)\n",
    "#al_dataset.label_randomly(NO_OF_INITIAL_LABELLED)  # Start with 20 items labelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(784, 256),\n",
    "    nn.Dropout(p=0.25),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 256),\n",
    "    nn.Dropout(p=0.25),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10),\n",
    "    nn.Softmax(dim=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = patch_module(model)\n",
    "if use_cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper = ModelWrapper(model=model, criterion=nn.CrossEntropyLoss())\n",
    "#wrapper.metrics = dict()\n",
    "#wrapper.add_metric(\"accuracy\", Accuracy)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bald = BALD()\n",
    "\n",
    "al_loop = ActiveLearningLoop(\n",
    "    dataset=al_dataset,\n",
    "    get_probabilities=wrapper.predict_on_dataset,\n",
    "    heuristic=bald,\n",
    "    ndata_to_label=10,  # We will label 100 examples per step.\n",
    "    # KWARGS for predict_on_dataset\n",
    "    iterations=20,  # 20 sampling for MC-Dropout\n",
    "    batch_size=BATCH_SIZE,\n",
    "    use_cuda=use_cuda,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11436-MainThread] [baal.modelwrapper:train_on_dataset:116] 2021-11-16T20:52:36.120725Z [info     ] Starting training              dataset=20 epoch=50\n",
      "[11436-MainThread] [baal.modelwrapper:train_on_dataset:127] 2021-11-16T20:55:19.786840Z [info     ] Training complete              train_accuracy=0.6500000357627869\n",
      "[11436-MainThread] [baal.modelwrapper:test_on_dataset:155] 2021-11-16T20:55:19.800192Z [info     ] Starting evaluating            dataset=10000\n",
      "[11436-MainThread] [baal.modelwrapper:test_on_dataset:165] 2021-11-16T20:55:22.609075Z [info     ] Evaluation complete            test_accuracy=0.41653481125831604\n",
      "{'dataset_size': 20,\n",
      " 'test_accuracy': 0.41653481125831604,\n",
      " 'train_accuracy': 0.6500000357627869}\n",
      "[11436-MainThread] [baal.modelwrapper:predict_on_dataset_generator:263] 2021-11-16T20:55:22.677113Z [info     ] Start Predict                  dataset=59980\n",
      "[11436-MainThread] [baal.modelwrapper:train_on_dataset:116] 2021-11-16T20:55:45.217340Z [info     ] Starting training              dataset=30 epoch=50\n"
     ]
    }
   ],
   "source": [
    "for step in range(10):\n",
    "    model.load_state_dict(initial_weights)\n",
    "    train_loss = wrapper.train_on_dataset(\n",
    "        al_dataset, optimizer=optimizer, batch_size=BATCH_SIZE, epoch=EPOCH, use_cuda=use_cuda\n",
    "    )\n",
    "    test_loss = wrapper.test_on_dataset(test_ds, batch_size=BATCH_SIZE, use_cuda=use_cuda)\n",
    "\n",
    "    pprint(\n",
    "        {\n",
    "            \"dataset_size\": len(al_dataset),\n",
    "            #\"train_loss\": wrapper.metrics[\"train_loss\"].value,\n",
    "            #\"test_loss\": wrapper.metrics[\"test_loss\"].value,\n",
    "            \"train_accuracy\": wrapper.metrics['train_accuracy'].value,\n",
    "            \"test_accuracy\": wrapper.metrics['test_accuracy'].value,\n",
    "        }\n",
    "    )\n",
    "    flag = al_loop.step()\n",
    "    if not flag:\n",
    "        # We are done labelling! stopping\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msthesisenv",
   "language": "python",
   "name": "msthesisenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
